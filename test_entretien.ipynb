{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA-C7GIlQJhx",
        "outputId": "eb50ba7d-7748-44fd-f125-92abfcdfa76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# 1. Créer une session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Pipeline PySpark Test\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Lire inverter_yields.csv\n",
        "df_inverter_yields = spark.read.csv(\"inverter_yields.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Lire site_median_reference.csv\n",
        "df_site_median = spark.read.csv(\"site_median_reference.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Lire sldc_events.csv\n",
        "df_sldc_events = spark.read.csv(\"sldc_events.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Lire static_inverter_info.csv\n",
        "df_static_inverter = spark.read.csv(\"static_inverter_info.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#df_inverter_yields.show(5)\n",
        "#df_site_median.show(5)\n",
        "#df_sldc_events.show(5)\n",
        "#df_static_inverter.show(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "6BEHRreyQLwr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJvY02WmQvA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Kte_e8FMS92U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joints inverter_yields avec :static_inverter_info via logical_device_mrid\n",
        "df_joined = df_inverter_yields.join(\n",
        "    df_static_inverter,\n",
        "    on=\"logical_device_mrid\"  # colonne commune\n",
        "    #how=\"inner\"                # inner join\n",
        ")\n",
        "df_joined.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OBZpk-KTCDj",
        "outputId": "88444bdf-a996-4e84-9866-833d1860ede9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+----------------------+---------------------+------------+\n",
            "|logical_device_mrid|           ts_start|             ts_end|project_code|specific_yield_ac|reference_yield_stc|project_mrid|inverter_function_type|storage_inverter_type|ac_max_power|\n",
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+----------------------+---------------------+------------+\n",
            "|             INV_A1|2025-06-01 00:00:00|2025-06-01 00:10:00|      SITE_A|            1.188|              1.095|       PRJ_A|                    PV|           DC-Coupled|       15000|\n",
            "|             INV_A2|2025-06-01 00:00:00|2025-06-01 00:10:00|      SITE_A|            1.014|              1.337|       PRJ_A|                    PV|                 NULL|       16000|\n",
            "|             INV_B1|2025-06-01 00:00:00|2025-06-01 00:10:00|      SITE_B|            0.843|              1.077|       PRJ_B|               Storage|           AC-Coupled|       14000|\n",
            "|             INV_A1|2025-06-01 00:10:00|2025-06-01 00:20:00|      SITE_A|            1.088|              1.421|       PRJ_A|                    PV|           DC-Coupled|       15000|\n",
            "|             INV_A2|2025-06-01 00:10:00|2025-06-01 00:20:00|      SITE_A|             1.03|              1.276|       PRJ_A|                    PV|                 NULL|       16000|\n",
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+----------------------+---------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Joints inverter_yields avec : sldc_events en chevauchement temporel\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "# Convertir les colonnes en timestamp\n",
        "df_inverter_yields = df_inverter_yields.withColumn(\"ts_start\", col(\"ts_start\").cast(TimestampType()))\n",
        "df_inverter_yields = df_inverter_yields.withColumn(\"ts_end\", col(\"ts_end\").cast(TimestampType()))\n",
        "\n",
        "df_sldc_events = df_sldc_events.withColumn(\"ts_start\", col(\"ts_start\").cast(TimestampType()))\n",
        "df_sldc_events = df_sldc_events.withColumn(\"ts_end\", col(\"ts_end\").cast(TimestampType()))\n",
        "\n",
        "df_time_joined = df_inverter_yields.join(\n",
        "    df_sldc_events,\n",
        "    (df_inverter_yields.ts_start <= df_sldc_events.ts_end) &\n",
        "    (df_sldc_events.ts_start <= df_inverter_yields.ts_end)\n",
        "    #how=\"inner\"\n",
        ")\n",
        "df_time_joined.show(5)\n",
        "df_time_joined.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsjacmQxTFLX",
        "outputId": "7305e08c-bca2-47cb-a051-b708ff7ac5f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+\n",
            "|           ts_start|             ts_end|logical_device_mrid|project_code|specific_yield_ac|reference_yield_stc|project_mrid|logical_device_mrid|           ts_start|             ts_end|iec63019_category_id|\n",
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+\n",
            "|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A1|      SITE_A|            1.188|              1.095|       PRJ_A|             INV_A1|2025-06-01 00:00:00|2025-06-01 00:20:00|     FULL_CAPABILITY|\n",
            "|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A1|      SITE_A|            1.188|              1.095|       PRJ_A|             INV_A2|2025-06-01 00:00:00|2025-06-01 00:20:00|     FULL_CAPABILITY|\n",
            "|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A1|      SITE_A|            1.188|              1.095|       PRJ_A|             INV_B1|2025-06-01 00:00:00|2025-06-01 00:20:00|     FULL_CAPABILITY|\n",
            "|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A2|      SITE_A|            1.014|              1.337|       PRJ_A|             INV_A1|2025-06-01 00:00:00|2025-06-01 00:20:00|     FULL_CAPABILITY|\n",
            "|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A2|      SITE_A|            1.014|              1.337|       PRJ_A|             INV_A2|2025-06-01 00:00:00|2025-06-01 00:20:00|     FULL_CAPABILITY|\n",
            "+-------------------+-------------------+-------------------+------------+-----------------+-------------------+------------+-------------------+-------------------+-------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- ts_start: timestamp (nullable = true)\n",
            " |-- ts_end: timestamp (nullable = true)\n",
            " |-- logical_device_mrid: string (nullable = true)\n",
            " |-- project_code: string (nullable = true)\n",
            " |-- specific_yield_ac: double (nullable = true)\n",
            " |-- reference_yield_stc: double (nullable = true)\n",
            " |-- project_mrid: string (nullable = true)\n",
            " |-- logical_device_mrid: string (nullable = true)\n",
            " |-- ts_start: timestamp (nullable = true)\n",
            " |-- ts_end: timestamp (nullable = true)\n",
            " |-- iec63019_category_id: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Joints inverter_yields avec :site_median_reference sur project_code et ts_start\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import TimestampType\n",
        "\n",
        "# Conversion ts_start pour les deux DataFrames\n",
        "df_inverter_yields = df_inverter_yields.withColumn(\"ts_start\", col(\"ts_start\").cast(TimestampType()))\n",
        "df_site_median = df_site_median.withColumn(\"ts_start\", col(\"ts_start\").cast(TimestampType()))\n",
        "\n",
        "df_median_joined = df_inverter_yields.join(\n",
        "    df_site_median,\n",
        "    on=[\"project_code\", \"ts_start\"],  # jointure sur deux colonnes\n",
        "    how=\"inner\"                       # inner join\n",
        ")\n",
        "\n",
        "# verify result\n",
        "df_median_joined.show(5)\n",
        "df_median_joined.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHp9trOcVEbc",
        "outputId": "61ae4a33-114c-417a-b8df-1e0d02dcef9f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------+--------------------------+\n",
            "|project_code|           ts_start|             ts_end|logical_device_mrid|specific_yield_ac|reference_yield_stc|project_mrid|site_median_specific_yield|\n",
            "+------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------+--------------------------+\n",
            "|      SITE_A|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A1|            1.188|              1.095|       PRJ_A|                       1.1|\n",
            "|      SITE_A|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_A2|            1.014|              1.337|       PRJ_A|                       1.1|\n",
            "|      SITE_B|2025-06-01 00:00:00|2025-06-01 00:10:00|             INV_B1|            0.843|              1.077|       PRJ_B|                       1.2|\n",
            "+------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------+--------------------------+\n",
            "\n",
            "root\n",
            " |-- project_code: string (nullable = true)\n",
            " |-- ts_start: timestamp (nullable = true)\n",
            " |-- ts_end: timestamp (nullable = true)\n",
            " |-- logical_device_mrid: string (nullable = true)\n",
            " |-- specific_yield_ac: double (nullable = true)\n",
            " |-- reference_yield_stc: double (nullable = true)\n",
            " |-- project_mrid: string (nullable = true)\n",
            " |-- site_median_specific_yield: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcule potential_production = specific_yield_ac × ac_max_power × 1/6 (10min en heures)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_joined = df_joined.withColumn(\n",
        "    \"potential_production\",\n",
        "    col(\"specific_yield_ac\") * col(\"ac_max_power\") * (1/6)\n",
        ")\n",
        "#verify result\n",
        "df_joined.select(\"logical_device_mrid\", \"specific_yield_ac\", \"ac_max_power\", \"potential_production\").show(5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI4C3kzgWnPD",
        "outputId": "67da866c-fb95-42de-eca5-71d0edbe969f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+------------+--------------------+\n",
            "|logical_device_mrid|specific_yield_ac|ac_max_power|potential_production|\n",
            "+-------------------+-----------------+------------+--------------------+\n",
            "|             INV_A1|            1.188|       15000|              2970.0|\n",
            "|             INV_A2|            1.014|       16000|              2704.0|\n",
            "|             INV_B1|            0.843|       14000|              1967.0|\n",
            "|             INV_A1|            1.088|       15000|              2720.0|\n",
            "|             INV_A2|             1.03|       16000|  2746.6666666666665|\n",
            "+-------------------+-----------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ne conserve que les inverters \"PV\" qui ne sont pas \"AC-Coupled\"\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_filtered = df_joined.filter(\n",
        "    (col(\"inverter_function_type\") == \"PV\") &\n",
        "    (col(\"storage_inverter_type\") != \"AC-Coupled\")\n",
        ")\n",
        "df_filtered.select(\"logical_device_mrid\", \"inverter_function_type\", \"storage_inverter_type\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyz4uwD6YVxo",
        "outputId": "fcf00223-02ef-42e5-87c5-885966669de6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------------+---------------------+\n",
            "|logical_device_mrid|inverter_function_type|storage_inverter_type|\n",
            "+-------------------+----------------------+---------------------+\n",
            "|             INV_A1|                    PV|           DC-Coupled|\n",
            "|             INV_A1|                    PV|           DC-Coupled|\n",
            "|             INV_A1|                    PV|           DC-Coupled|\n",
            "|             INV_A1|                    PV|           DC-Coupled|\n",
            "|             INV_A1|                    PV|           DC-Coupled|\n",
            "+-------------------+----------------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Produit un fichier parquet partitionné par project_code et year_month\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "# Créer la colonne year_month à partir de ts_start\n",
        "from pyspark.sql.functions import date_format, col\n",
        "\n",
        "df_final = df_filtered.withColumn(\"year_month\", date_format(col(\"ts_start\"), \"yyyy-MM\"))\n",
        "\n",
        "# Écrire le fichier Parquet partitionné à la racine de Colab\n",
        "df_final.write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"project_code\", \"year_month\") \\\n",
        "    .parquet(\"/content/parquet_data\")\n"
      ],
      "metadata": {
        "id": "NIVMOWXoZXl9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/parquet_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsITEQ1NaMmj",
        "outputId": "14d369ed-4cd3-4d7c-abaf-b48de873d76a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'project_code=SITE_A'   _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test spark\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def run_tests(df):\n",
        "    \"\"\"\n",
        "    Exécute des tests de validation sur le DataFrame final du pipeline PySpark.\n",
        "    \"\"\"\n",
        "    # Vérifier que le DataFrame n'est pas vide\n",
        "    assert df.count() > 0, \"Le DataFrame final est vide !\"\n",
        "\n",
        "    # Vérifier la présence des colonnes importantes\n",
        "    expected_columns = [\n",
        "        \"logical_device_mrid\",\n",
        "        \"ts_start\",\n",
        "        \"ts_end\",\n",
        "        \"specific_yield_ac\",\n",
        "        \"ac_max_power\",\n",
        "        \"potential_production\",\n",
        "        \"inverter_function_type\",\n",
        "        \"storage_inverter_type\"\n",
        "    ]\n",
        "    for col_name in expected_columns:\n",
        "        assert col_name in df.columns, f\"La colonne {col_name} est manquante dans le DataFrame final\"\n",
        "\n",
        "    # Vérifier que potential_production > 0\n",
        "    zero_potential = df.filter(col(\"potential_production\") <= 0).count()\n",
        "    assert zero_potential == 0, f\"Il y a {zero_potential} lignes avec potential_production <= 0\"\n",
        "\n",
        "    # Vérifier le filtrage PV non AC-Coupled\n",
        "    non_pv = df.filter(col(\"inverter_function_type\") != \"PV\").count()\n",
        "    ac_coupled = df.filter(col(\"storage_inverter_type\") == \"AC-Coupled\").count()\n",
        "    assert non_pv == 0, f\"Il y a {non_pv} inverters qui ne sont pas PV\"\n",
        "    assert ac_coupled == 0, f\"Il y a {ac_coupled} inverters AC-Coupled\"\n",
        "\n",
        "    # Vérifier la présence de partitions project_code et year_month\n",
        "    if \"project_code\" in df.columns and \"year_month\" in df.columns:\n",
        "        partitions = df.select(\"project_code\", \"year_month\").distinct().count()\n",
        "        assert partitions > 0, \"Aucune partition project_code/year_month détectée\"\n",
        "\n",
        "    print(\"Tous les tests Spark ont été passés avec succès !\")\n"
      ],
      "metadata": {
        "id": "fWSt9TeHbDlM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_tests(df_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvBkk77LcSek",
        "outputId": "bfc2df60-78f4-4424-b87e-79d7ddc476b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tous les tests Spark ont été passés avec succès !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzkSAS63ce-j"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline pyspark final, complet et portable\n",
        "!pip install pyspark\n",
        "\n",
        "# --------------------------\n",
        "# Importer PySpark et fonctions\n",
        "# --------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# --------------------------\n",
        "# Paramètres\n",
        "# --------------------------\n",
        "CSV_FILES = {\n",
        "    \"inverter_yields\": \"inverter_yields.csv\",\n",
        "    \"static_inverter_info\": \"static_inverter_info.csv\",\n",
        "    \"sldc_events\": \"sldc_events.csv\",\n",
        "    \"site_median_reference\": \"site_median_reference.csv\"\n",
        "}\n",
        "\n",
        "OUTPUT_PATH = \"/content/parquet_data\"\n",
        "\n",
        "# --------------------------\n",
        "# Initialiser Spark\n",
        "# --------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"InverterPipeline\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --------------------------\n",
        "# Lire les CSV\n",
        "# --------------------------\n",
        "df_inverter_yields = spark.read.csv(CSV_FILES[\"inverter_yields\"], header=True, inferSchema=True)\n",
        "df_static_inverter = spark.read.csv(CSV_FILES[\"static_inverter_info\"], header=True, inferSchema=True)\n",
        "df_sldc_events = spark.read.csv(CSV_FILES[\"sldc_events\"], header=True, inferSchema=True)\n",
        "df_site_median = spark.read.csv(CSV_FILES[\"site_median_reference\"], header=True, inferSchema=True)\n",
        "\n",
        "# --------------------------\n",
        "# Convertir colonnes timestamp\n",
        "# --------------------------\n",
        "df_inverter_yields = df_inverter_yields.withColumn(\"ts_start_inv\", F.col(\"ts_start\").cast(\"timestamp\")) \\\n",
        "                                       .withColumn(\"ts_end_inv\", F.col(\"ts_end\").cast(\"timestamp\"))\n",
        "\n",
        "df_sldc_events = df_sldc_events.withColumn(\"ts_start_event\", F.col(\"ts_start\").cast(\"timestamp\")) \\\n",
        "                               .withColumn(\"ts_end_event\", F.col(\"ts_end\").cast(\"timestamp\"))\n",
        "\n",
        "df_site_median = df_site_median.withColumn(\"ts_start_median\", F.col(\"ts_start\").cast(\"timestamp\"))\n",
        "\n",
        "# --------------------------\n",
        "# Renommer colonnes static_inverter\n",
        "# --------------------------\n",
        "df_static_inverter = df_static_inverter.withColumnRenamed(\"logical_device_mrid\", \"logical_device_mrid_static\")\n",
        "\n",
        "# --------------------------\n",
        "# Jointures avec alias pour éviter ambiguïté\n",
        "# --------------------------\n",
        "\n",
        "# 7.1 inverter_yields + static_inverter_info\n",
        "left = df_inverter_yields.alias(\"inv\")\n",
        "right = df_static_inverter.alias(\"stat\")\n",
        "\n",
        "df_joined = left.join(\n",
        "    right,\n",
        "    F.col(\"inv.logical_device_mrid\") == F.col(\"stat.logical_device_mrid_static\"),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    F.col(\"inv.logical_device_mrid\").alias(\"logical_device_mrid\"),\n",
        "    F.col(\"inv.ts_start_inv\"),\n",
        "    F.col(\"inv.ts_end_inv\"),\n",
        "    F.col(\"inv.specific_yield_ac\"),\n",
        "    F.col(\"inv.project_code\"),\n",
        "    F.col(\"stat.ac_max_power\"),\n",
        "    F.col(\"stat.inverter_function_type\"),\n",
        "    F.col(\"stat.storage_inverter_type\")\n",
        ")\n",
        "\n",
        "# 7.2 Jointure temporelle avec sldc_events\n",
        "left = df_joined.alias(\"left\")\n",
        "right = df_sldc_events.alias(\"right\")\n",
        "\n",
        "df_joined2 = left.join(\n",
        "    right,\n",
        "    (F.col(\"left.ts_start_inv\") <= F.col(\"right.ts_end_event\")) &\n",
        "    (F.col(\"right.ts_start_event\") <= F.col(\"left.ts_end_inv\")),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    F.col(\"left.logical_device_mrid\"),\n",
        "    F.col(\"left.ts_start_inv\"),\n",
        "    F.col(\"left.ts_end_inv\"),\n",
        "    F.col(\"left.specific_yield_ac\"),\n",
        "    F.col(\"left.project_code\"),\n",
        "    F.col(\"left.ac_max_power\"),\n",
        "    F.col(\"left.inverter_function_type\"),\n",
        "    F.col(\"left.storage_inverter_type\")\n",
        ")\n",
        "\n",
        "# 7.3 Jointure avec site_median_reference sur project_code et ts_start\n",
        "left = df_joined2.alias(\"left\")\n",
        "right = df_site_median.alias(\"right\")\n",
        "\n",
        "df_joined3 = left.join(\n",
        "    right,\n",
        "    (F.col(\"left.project_code\") == F.col(\"right.project_code\")) &\n",
        "    (F.col(\"left.ts_start_inv\") == F.col(\"right.ts_start_median\")),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    F.col(\"left.logical_device_mrid\"),\n",
        "    F.col(\"left.ts_start_inv\"),\n",
        "    F.col(\"left.ts_end_inv\"),\n",
        "    F.col(\"left.specific_yield_ac\"),\n",
        "    F.col(\"left.project_code\"),\n",
        "    F.col(\"left.ac_max_power\"),\n",
        "    F.col(\"left.inverter_function_type\"),\n",
        "    F.col(\"left.storage_inverter_type\")\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Calcul potential_production\n",
        "# --------------------------\n",
        "df_joined3 = df_joined3.withColumn(\n",
        "    \"potential_production\",\n",
        "    F.col(\"specific_yield_ac\") * F.col(\"ac_max_power\") * (1/6)\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Filtrer PV non AC-Coupled\n",
        "# --------------------------\n",
        "df_filtered = df_joined3.filter(\n",
        "    (F.col(\"inverter_function_type\") == \"PV\") &\n",
        "    (F.col(\"storage_inverter_type\") != \"AC-Coupled\")\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Créer year_month et sélectionner colonnes finales\n",
        "# --------------------------\n",
        "df_final_clean = df_filtered.withColumn(\n",
        "    \"year_month\", F.date_format(F.col(\"ts_start_inv\"), \"yyyy-MM\")\n",
        ").select(\n",
        "    F.col(\"logical_device_mrid\"),\n",
        "    F.col(\"ts_start_inv\").alias(\"ts_start\"),\n",
        "    F.col(\"ts_end_inv\").alias(\"ts_end\"),\n",
        "    F.col(\"specific_yield_ac\"),\n",
        "    F.col(\"ac_max_power\"),\n",
        "    F.col(\"potential_production\"),\n",
        "    F.col(\"inverter_function_type\"),\n",
        "    F.col(\"storage_inverter_type\"),\n",
        "    F.col(\"project_code\"),\n",
        "    F.col(\"year_month\")\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "#  Écrire Parquet partitionné\n",
        "# --------------------------\n",
        "df_final_clean.write.mode(\"overwrite\").partitionBy(\"project_code\", \"year_month\").parquet(OUTPUT_PATH)\n",
        "\n",
        "# --------------------------\n",
        "# Fonction de tests Spark\n",
        "# --------------------------\n",
        "def run_tests(df):\n",
        "    assert df.count() > 0, \"Le DataFrame final est vide !\"\n",
        "    expected_columns = [\n",
        "        \"logical_device_mrid\",\"ts_start\",\"ts_end\",\"specific_yield_ac\",\n",
        "        \"ac_max_power\",\"potential_production\",\"inverter_function_type\",\n",
        "        \"storage_inverter_type\",\"project_code\",\"year_month\"\n",
        "    ]\n",
        "    for col_name in expected_columns:\n",
        "        assert col_name in df.columns, f\"La colonne {col_name} est manquante\"\n",
        "    zero_potential = df.filter(F.col(\"potential_production\") <= 0).count()\n",
        "    assert zero_potential == 0, f\"Il y a {zero_potential} lignes avec potential_production <= 0\"\n",
        "    non_pv = df.filter(F.col(\"inverter_function_type\") != \"PV\").count()\n",
        "    ac_coupled = df.filter(F.col(\"storage_inverter_type\") == \"AC-Coupled\").count()\n",
        "    assert non_pv == 0, f\"Il y a {non_pv} inverters qui ne sont pas PV\"\n",
        "    assert ac_coupled == 0, f\"Il y a {ac_coupled} inverters AC-Coupled\"\n",
        "    partitions = df.select(\"project_code\", \"year_month\").distinct().count()\n",
        "    assert partitions > 0, \"Aucune partition project_code/year_month détectée\"\n",
        "    print(\"✅ Tous les tests Spark ont été passés avec succès !\")\n",
        "\n",
        "# --------------------------\n",
        "# Lancer les tests\n",
        "# --------------------------\n",
        "run_tests(df_final_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVYR9dM5dzpq",
        "outputId": "6502b096-f12f-40c3-cbd0-5974e3cc4b5b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "✅ Tous les tests Spark ont été passés avec succès !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPfzg-g9gqBW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6s3xAI88w3Wk"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-aCUQawxHkl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QRTfMqQxMBA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gDcAHKsxrLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}